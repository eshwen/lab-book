
\chapter{Combined \texorpdfstring{$H \rightarrow \mathrm{inv.}$}{Higgs to invisible} analysis (25/05/2018)}

\section{Motivation}

Whilst working on semi-visible jets, I am also involved in a combined Higgs to invisible analysis. This is "combined" in the sense that we will perform a Higgs $\rightarrow$ inv. search over all possible SM production modes in a single analysis. This is in contrast to the regular approach in which a separate analysis would be conducted for each channel and results would be combined at the end. Our strategy should give much better sensitivity than has previously been possible.

Despite this analysis seeming purely Standard Model-based, there is the possibility to explore new physics. The branching fraction for $H \rightarrow \nu\nu$ is predicted to be approximately 0.1\%~\cite{Heinemeyer:1559921}. However, the current experimental limit on this value is 24\%~\cite{Khachatryan:2016whc} (\textbf{update: now 19\%}~\cite{Sirunyan:2018owy}). If the Higgs couples to new, exotic particles, this branching ratio should increase as the coupling is related to the mass of the particle. If we are able to close the gap between the theoretical and experimental values, it may prove to be a powerful constraint on new physics models. Once the analysis has been finalised, new models can be considered such as those involving dark matter.

In the SM, the Higgs can only decay to neutrinos via $H \rightarrow ZZ \rightarrow 4\nu$, with a predicted branching ratio of $1.06 \times 10^{-3}$~\cite{Heinemeyer:1559921}. It may also decay directly to neutrinos ($H \rightarrow \nu\nu$) if neutrinos acquire their mass from the Higgs. Although, as neutrino masses (if they do indeed have mass) are very small, this decay would be suppressed.


\section{Models and sample production}

A subset of the Higgs production modes we are considering are gluon-gluon fusion ($gg$F), top pair production with a Higgs ($ttH$), Higgs production in association with a vector boson ($VH$), and vector boson fusion (VBF). Our first step is to analyse samples from each of these processes and attempt to construct orthogonal signal regions to cover the largest phase space possible. An analysis will then be performed to extract the limit on the invisible (neutrino) branching fraction. As these processes are Standard Model and not BSM, this is a simpler undertaking in the sense it does not involve parameter scans or any other nuisances that arises from those models.

My current task is to investigate $ttH$ sample production, binning it, and exploring an event selection and discriminating variables. In the future, it may develop into analysing dark matter models in this context, if we decide on pursuing BSM physics. For all of the modes above, we plan to use the centrally-produced nanoAOD MC However, for $ttH$, they haven't been produced yet as of 30/05/2018 (but the request has gone in, so it should be available soon). The dataset entry is on McM though, so I could see the CMSSW version and cmsDriver command required to produce my own samples from the miniAODs for the time being. So, I made some nanoAOD files which I could run through FAST-RA1. (A few people on this analysis are actively involved in developing FAST, and this framework will very likely be what we use in the analysis.)


\subsection{Building the dataframe of files to run over}

I needed to build a list of files to run over, which is straightforward if they're local but more involved if they're not. I could use \texttt{t2df\_find\_files.py}, with the option of an output text file containing the list to use in the next steps. The argument I needed to specify was the actual files, which allows wildcarding. If I looked up the dataset on DAS and clicked on "files", I could use the prefix \texttt{root://cms-xrd-global.cern.ch//} in the argument, then append the file paths "/store/.../*.root". This would use AAA with the global redirector to look for the file names. So the full command would look like this:

\begin{lstlisting}[belowskip=-0.7cm, language=sh, numbers=none]
bin/t2df_find_files.py -d /TTJets_HT-2500toInf_TuneCUETP8M1_13TeV-madgraphMLM-pythia8/RunIISummer16NanoAOD-PUMoriond17_05Feb2018_94X_mcRun2_asymptotic_v2_ext1-v1/NANOAODSIM -o file_list_ttJets_2500toInf.txt --mc root://cms-xrd-global.cern.ch///store/mc/RunIISummer16NanoAOD/TTJets_HT-2500toInf_TuneCUETP8M1_13TeV-madgraphMLM-pythia8/NANOAODSIM/PUMoriond17_05Feb2018_94X_mcRun2_asymptotic_v2_ext1-v1/00000/*.root
\end{lstlisting}

If that doesn't work, as sometimes things the file query can hang, I can instead use Shane's \texttt{cms-das-query} package in \textbf{external/}. I just need to supply the dataset name, like the following:

\begin{lstlisting}[belowskip=-0.7cm, language=sh, numbers=none]
bin/das_query --do-xsdb-query -f pandas -o ${FAST_RA1_\ROOT}/file_list_ttJets_2500toInf.txt /TTJets_HT-2500toInf_TuneCUETP8M1_13TeV-madgraphMLM-pythia8/RunIISummer16NanoAOD-PUMoriond17_05Feb2018_94X_mcRun2_asymptotic_v2_ext1-v1/NANOAODSIM
\end{lstlisting}

which also gives me an output dataframe with the list of files. If I want to run over multiple datasets, I can specify the same output file. The information from the new datasets will be appended it. If I want to run over multiple datasets for a specific process, I can use a loop. If I add each dataset name to a file, say \textbf{qcd\_bkg\_datasets.dat}, I can run

\begin{lstlisting}[belowskip=-0.7cm, language=sh, numbers=none]
for dataset in $(cat qcd_bkg_datasets.dat); do bin/das_query --do-xsdb-query -f pandas -o ./qcd_bkg_file_list.txt $dataset; done
\end{lstlisting}

Note that if using \texttt{das\_query}, I need to source the setup script in that directory, make sure my grid certificate proxy is active, make sure the \textbf{xsecdb} submodule is cloned, and install the pip packages in the requirements file.


\subsection{Building the dataframes from nanoAODs}

Then, I could specify binning and an event selection in a YAML config file, then I could run \texttt{nanoaod2dataframes\_cfg} to get a dataframe. I can look at the other configs to get an idea of the layout and arguments, and then the actual executable for its arguments and options. So an example command would be

\begin{lstlisting}[belowskip=-0.7cm, language=sh, numbers=none]
bin/nanoaod2dataframes_cfg --components file_list_ttJets_2500toInf.txt --dont-use-nanoaodtools -o single_top_test fast_ra1/trees_to_dataframe/configs/nanoaod_esh_ttH.yaml
\end{lstlisting}

If the file names from the previous step start with "/store", I need to enable my grid certificate proxy and add the option \texttt{--xrd-redirector root://cms-xrd-global.cern.ch//} so the executable knows to prepend the redirector on to them. If running over many files, it is preferable to run on Condor rather than locally. I can specify the option \texttt{--mode htcondor}, but if running over files with xrootd, I need to initialise my grid certificate proxy somewhere that the Condor jobs can access (i.e., on \textbf{/afs} rather than the default \textbf{/tmp}). If I run the commands

\begin{lstlisting}[belowskip=-0.7cm, language=sh, numbers=none]
export X509_USER_PROXY=/afs/cern.ch/user/e/ebhal/x509up_u${UID}
voms-proxy-init --voms cms --valid 168:00
\end{lstlisting}

A proxy will be saved in my home directory and, assuming \texttt{getenv = True} in the Condor job files, everything should work. When doing this often, it can get tedious having to enter my password all the time. I wrote a little script that only refreshes my proxy if required:

\begin{lstlisting}[belowskip=-0.7cm, language=sh, numbers=none]
export X509_USER_PROXY=/afs/cern.ch/user/e/ebhal/x509up_u${UID}
inception=$(( $(date "+%s") - $(date "+%s" -r ${X509_USER_PROXY}) ))
proxy_length_hr=168 # 1 week                                                                               
proxy_length_s=$(( $proxy_length_hr * 3600 ))
if (( "$inception" < "$proxy_length_s" )); then
    echo "No need to re-initialise proxy as it's still valid"
else
    voms-proxy-init --voms cms --valid ${proxy_length_hr}:00
fi
\end{lstlisting}

On Condor, the monitoring from AlphaTwirl needs to stay alive until all jobs finish. This makes it preferable to run everything in a \texttt{screen} session. One problem is that if I detach the session for too long, the kerberos authentication token expires and the whole thing will error out. By following this link: \url{https://lhcb.github.io/analysis-essentials/shell-extras/persistent-screen.html} (making sure the "CERN.CH" is capitalised when I try), I can set up screen sessions that refresh the token every so often. \uline{If my CERN account password changes, I'll have to repeat the steps with my new password}. So my basic workflow for this step would look like this:

\begin{lstlisting}[belowskip=-0.7cm, language=sh, numbers=none]
<make sure Kerberos keytab has been generated with link above. Only needs to be done once>
k5reauth -f -i 1200 -p ebhal -k ~/.krb5_tokens/ebhal.keytab -- screen
kinit
export X509_USER_PROXY=/afs/cern.ch/user/e/ebhal/x509up_u${UID}
voms-proxy-init --voms cms --valid 168:00
cd <path>/FAST-RA1
source setup.sh
bin/nanoaod2dataframes_cfg --mode htcondor <other arguments/options>
\end{lstlisting}

which will get things running.

If Condor jobs fail whilst the process is running, they will be automatically resubmitted, but \emph{not} if they're held. In these cases, I can run

\begin{lstlisting}[belowskip=-0.7cm, language=sh, numbers=none]
while true; do date; jobs="$(condor_q -hold $USER -format "%d." ClusterId -format "%d " ProcId)"; [ -n "$jobs" ] && condor_release $jobs; sleep 120; done
\end{lstlisting}

in a separate window, also in a screen session for good measure. 

The output from \texttt{nanoaod2dataframes\_cfg} will be a dataframe with the events that pass the selections specified in the config file, and binned according to it. These can be loaded and plotted using \texttt{pandas} and \texttt{matplotlib}. FAST doesn't have any plotting functions built in, but those two packages are fairly straightforward to use. 


\subsection{Cross section reweighting}

When skimming over datasets (e.g., if you want to apply some pre-selection and reduce its overall footprint), reweighting for the cross section needs to be taken into account. To first order, the weight applied is

\begin{equation}
w = \frac{ \sigma \lumi }{ N_{\mathrm{tot.}} \varepsilon }
\end{equation}

where $\sigma$ is the cross section of the dataset (on DAS/XSDB for public datasets), $\lumi$ is the integrated luminosity, $N_{\mathrm{tot.}}$ is the number of events in the dataset before any skimming, and $\varepsilon$ is the filter efficiency.

For datasets that are split, e.g., by HT (like QCD or Drell-Yan), it's more complex as the weight needs to take all the datasets over the range into account. For Drell-Yan, the new calculation would be

\begin{equation}
w_i = \frac{ \sigma_i ' \lumi }{ N_{\mathrm{tot.}} \varepsilon }, \ \mathrm{where} \ \sigma_i ' = \left( \frac{\sigma_i}{\Sigma_i \sigma_i} \right) \sigma_{\mathrm{tot.@NLO}}
\end{equation}

where the weight for a split dataset $i$ needs to be corrected by normalising the cross section by the sum of the cross sections in all the datasets (assuming the split datasets are contiguous, i.e., HT100-200, HT200-400, etc.), then multiplied by the total NLO cross section.


\section{Skimming over datasets}
The new FAST framework, written from the ground up to be fully vectorised (reading in and running over a "chunk" of several thousand events at a time as opposed to a single-event loop), is much quicker than FAST-RA1. The bottleneck, in terms of processing time, is now in I/O. When performing studies and tweaking various aspects of the analysis, we want to iterate as fast as possible. The easiest way to do this is by reducing the number of events that must be processed, i.e., by skimming over the files we eventually want to run.

NanoAOD-tools (\url{https://github.com/cms-nanoAOD/nanoAOD-tools}) has built-in support to run over nanoAOD files and run centrally-maintained modules (to add branches for various weights and object cleaning/ID) as well as custom ones. Our goal is to first run all the datasets we plan to use in FAST through nanoAOD-tools first, where we can add these extra weight and scale factor branches, perform some object cleaning, and apply a loose preselection (i.e., HLT paths and some filters). CRAB can be used to funnel the actual processing and allow us to store the output on /hdfs at Bristol. As we will be using FAST at Bristol, the I/O bottleneck is mitigated and the number of events to be processed is also reduced.

Vukasin and Ann-Marie have a setup going for their VBF-side processing, which I've forked: \url{https://github.com/eshwen/VBFHToInv-nanoAOD-tools}. I've written custom modules for various things and consolidated all the centralised modules we need. Then it's straightforward to run all the public datasets on CRAB and send the output to Bristol. The main problem was running over private datasets as CRAB only supports those accessible on DAS. For this, I had to write a Condor framework to process those and then manually transfer them to Bristol. To access the output files, one can use the local path if they're working on Soolin. But for external users, they need the Bristol xrootd redirector \texttt{root://lcgse01.phy.bris.ac.uk:11001//}. If the files were produced with CRAB, they can append onto that \texttt{/store/user/<user>/<path>/}. If they are instead produced another way (like on Condor and manually transferred across), they should instead append \texttt{<path minus "/hdfs">}.


\section{Analysis}

Once we have the skimmed samples, we can run them through the analysis software. This is essentially a new FAST-RA1, but rewritten from the ground up. All the code is stored in \url{https://gitlab.cern.ch/cms-chip/chip}. This stores the analysis-specific code (\textbf{C}ombined \textbf{H}iggs to \textbf{I}nvisible \textbf{P}roject) such as scribblers and sample lists. The machinery that runs everything is the package \texttt{fast\_carpenter}: \url{https://pypi.org/project/fast-carpenter/}. It takes in a YAML-style sequence cfg specifying each stage of the analysis like in FAST-RA1, and a YAML-style dataset cfg that takes in a list of files and any other metadata (cross section, etc.) that can be built with \texttt{fast\_curator} (https://pypi.org/project/fast-curator/). Behind the scenes, it uses \texttt{uproot} to convert flat trees into dataframes, \texttt{AlphaTwirl} for handling batch submission, and some other packages.

I've helped by developing modules and scribblers in the cms-chip repository, as well as making some plotting tools. All the code should be able to run a mostly-complete analysis from the sequence cfg, and includes support for weights, systematics as well as complex scribblers and other modules.

The analysis note is here: \url{http://cms.cern.ch/iCMS/jsp/db_notes/noteInfo.jsp?cmsnoteid=CMS%20AN-2018/299}. We also have a GitLab repo, migrated from the far-outdated SVN: \url{https://gitlab.cern.ch/tdr/notes/AN-18-299}. We can also edit using Overleaf, an online LaTeX editor. By running the commands

\begin{lstlisting}[belowskip=-0.7cm, language=sh, numbers=none]
git clone ssh://git@gitlab.cern.ch:7999/tdr/notes/AN-18-299
cd AN-18-299
git remote add overleaf https://git.overleaf.com/5cd2a70d3d57d07d68fa386a
git checkout -b overleaf_to_gitlab_v1
git pull overleaf master --allow-unrelated-histories
\end{lstlisting}

Whenever edits/updates are made to the project on Overleaf (\url{https://www.overleaf.com/project/5cd2a70d3d57d07d68fa386a}), it automatically updates the branch \texttt{overleaf/master}. By pulling the latest changes at a certain point in time, a merge request can then be made to the main GitLab repo above.

\section{IOP 2019 (25/01/2019)}

As I'm in my third year, I've been asked to present at the Joint APP and HEPP Annual Conference, organised by the IOP. I'll be giving a 15 minute talk on Higgs to invisible (as I've contributed to it more than semi-visible jets, recently). The title is \textbf{Combined Search for an Invisibly Decaying Higgs Boson in Hadronic Channels at $\sqrt{s} = 13$ TeV with CMS}, and the abstract is below:

The leading upper limit on the Higgs boson to invisible state branching ratio (BR) is 24\%, while the Standard Model prediction sits far below at 0.1\%. The observed value was measured using pp data collected by the CMS experiment between 2011 and 2015. Our analysis targets a better limit by using 13 TeV data from 2016-2018 -- an integrated luminosity of over 130\fbinv -- in addition to performing the combination over all Higgs production modes from the outset rather than in a posthoc fashion. The hadronic channels we include are gluon-gluon fusion, ttH, vector boson fusion (VBF) and Higgs production in association with a vector boson (VH). Analysing each production mode in an orthogonal search region gives a high degree of sensitivity compared to previous attempts. In this talk, the finalised event selection, signal categorisation, data-driven background estimation and systematic uncertainties for the non-VBF production modes will be presented. A sufficiently accurate limit on the BR that is still above the Standard Model prediction may be interpreted in a beyond-Standard Model context. Constraints can be placed on theories that posit exotic particles or dark matter that couple to the Higgs, enhancing the invisible state BR.

My finished slides are here: \href{run:./sec37/20190410 Combined Search for an Invisibly Decaying Higgs (final, 16x9).pdf}{20190410 Combined Search for an Invisibly Decaying Higgs (final, 16x9).pdf}. They include a very recent result for $ttH (H \rightarrow \mathrm{inv.})$:~\cite{CMS-PAS-HIG-18-008}, and also a combination from ATLAS (plots taken from conference note, paper uploaded soon after talk):~\cite{Aaboud:2019rtt}.


% --------------------------------- OTHER ---------------------------------
An interesting new publication from CMS about $ttH$, which may prove useful, is in Ref.~\cite{PhysRevLett.120.231801}.
When looking into 2016 and 2017 data in nanoAOD, I found a good overview into how data is recorded and managed in CMS: %\url{https://indico.cern.ch/event/455071/contributions/1123140/attachments/1229600/1801901/2016-02-17-cmsDasTaipei-franzoni-DataPreparation.pdf}. This is useful in general.
