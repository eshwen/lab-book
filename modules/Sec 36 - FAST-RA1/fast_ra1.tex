
\chapter{FAST-RA1 (01/02/2018)}
\label{sec:fast-ra1}

Over the last few months, Ben, Tai, Luke and Olivier (among others) have been working on a new framework to run data reduction and analysis. They called their team FAST (Faster Analysis Software Taskforce). Their first framework is FAST-RA1, which aims to replicate the RA1 analysis as validation to make sure the code works as it should. It is written in Python and relies mainly on pandas, NumPy, matplotlib and Tai's AlphaTwirl as dependencies. A tutorial was run on 1st Feb., and some PhD students in CMS (including myself) have been asked to help develop the code.

The slides shown can be found at \href{run:modules/Sec 36 - FAST-RA1/figures/FAST-RA1 Tutorial.pdf}{FAST-RA1 Tutorial}. The repository is hosted on GitLab for privacy and integration with CERN services at \url{https://gitlab.cern.ch/fast-cms/FAST-RA1}, which I've forked. The documentation, which is automatically kept up-to-date with the code, is currently located at \url{https://fast-hep.web.cern.ch/fast-hep/cms/fast-ra1/}. There's also a README on the main GitLab page, which is a condensed version of the important aspects. However, it may not be up to date. All the command-line options for each of the steps are detailed in \textbf{bin/$<$executable$>$}. 

Before cloning the repository, I had to generate new ssh keys for the remote servers I use; my Bristol email address is linked to GitHub, but GitLab requires my CERN account, so I couldn't just copy those SSH keys over. I could just follow the steps on \url{https://gitlab.cern.ch/help/ssh/README} to generate the key pairs and upload them to GitLab. As I'll be adding new SSH keys with potentially non-default filenames (e.g., \textbf{id\_rsa\_gitlab}), the instructions under "Working with non-default SSH key pair paths" are necessary. Then, to clone, there are multiple options I can choose from. If on lxplus, I could clone the URL under "KRB5", as lxplus has Kerberos authentication built in. But for Imperial and Soolin, I needed to clone the URL under "SSH". And so the submodules, etc., are picked up, I needed to include the \texttt{--recursive} option when cloning.

As is good practice, I will develop code and make my own branches, etc. in my own fork of the repo. But as the framework is being developed by lots of people and at a fast pace, I need to keep up with commits and new features. I shouldn't develop code on the master branch of my fork. That should be reserved for the latest revision of the code. So, to get the latest changes from the master branch of the head fork (which is where the latest, stable version of the code should live), I can follow the instructions at this link: \url{https://help.github.com/articles/syncing-a-fork/}. Should be as simple as doing \texttt{git fetch <remote> <branch>; git pull <remote> <branch>}, assuming there are no conflicts (which I can check with a \texttt{git diff}). Then, I can merge any of my code into my newly-updated master branch to see how well it plays with the latest code.


\section{Generating weighted cut flow tables}

If I want to make cut flow tables, I really only need to run the \texttt{trees2dataframes} step, making sure the event selection is correct. I could also use AlphaTools to calculate weights by cloning it:

\begin{lstlisting}[belowskip=-0.7cm, language=sh, numbers=none]
git clone --recursive git@github.com:CMSRA1/AlphaTools.git external/AlphaTools
\end{lstlisting}

Then I could specify the option \texttt{--alphatools} when running to include it and run the sequence. However, I had still had to make sure everything in AlphaTools was correct, like editing the base directory, location of the samples, adding the configuration file listing the samples, etc. But because I couldn't specify AlphaTools-like command-line arguments, I couldn't choose a pset (so didn't know which samples were being loaded). After some debugging, I realised that the samples loaded are from the \texttt{mcSRsamples} list. So if I added my collection of samples to that listed, they could be loaded. For accurate cut flow tables, I had to remove \texttt{skimSequence} from \texttt{sequence2016} and add the cut \texttt{ev : ev.failed\_alphatools[0] == "False"} at the start of the cut flow.

As I did in \emph{cutflowirl}, to run over event weights from AlphaTools for, e.g. weighted cut flow tables, I had to make some changes to the \emph{alphatwirl} and \emph{alphatwirl-interface} submodules. I added the relevant classes and made sure they were picked up by, and imported in, the right scripts in both submodules and FAST-RA1. I added the option in \textbf{trees2dataframes} to run over weights and get a weighted cut flow table out. The main annoyance was having to make commits in three repositories. In the submodules, I checked out new branches and committed there. Then, I could add the commit hash to my branch in FAST-RA1 (so that if I were to clone the repo at another site, everything would sync properly) with

\begin{lstlisting}[belowskip=-0.7cm, language=sh, numbers=none]
git commit external -m "<message>"
git push <remote> <branch in FAST-RA1>
\end{lstlisting}

so that the commits in the submodules would be picked up by my branch in FAST-RA1. For running over event weights, I had to edit some stuff in AlphaTools so the right trees would be picked up and run over. I also had to add the \texttt{uncertainties} module as a requirement in FAST-RA1 for un-pickling the cross section pickle files. Then, I could run \textbf{trees2dataframes} with \texttt{--alphatools --weighted} to generate weighted cut flow tables.


\section{Validating datacards produced by FAST-RA1}

One of my first tasks in contributing to FAST-RA1 was to validate the datacards produced in FAST-RA1. I needed to run those, with the shapes root files (containing the TH2Ds, equivalent to when applying AlphaTools), through AlphaStats to check they were correct. We used the T2tt-4bd (250,170) mass point as a test. Olivier took the split tree and made datacards/shapes, and I used the files I already had when running the analysis for that model. At each step, I compared the files in the directories to make sure everything was in order.

I could run \texttt{makeCardsAndWs} over the FAST datacards, which ended up being the most painful step. AlphaStats is quite rigid in the way it handles the datacards, so Olivier and I had to iterate to make sure the formatting, etc. was exactly correct. I also had to copy the pickle files (that are usually made during the \texttt{optimiseBinning} step) from the T2tt-4bd datacards I already had to the new folders. After that step succeeded, I could run \texttt{runCombineTask}. Making the limit plots for a visual comparison didn't work as the interpolation/plotting failed due to the fact we only had one mass point. However, we could compare the limit values in the datacards for a numerical comparison. We could also make limit-per-bin plots as they only require a single mass point. I made some slides giving an update this task: \href{run:modules/Sec 36 - FAST-RA1/figures/20180301 FAST-RA1 - Validating Datacards.pdf}{20180301 FAST-RA1 - Validating Datacards.pdf}.

We soon realised that we would need more mass points in order to generate the limit plane and then compare plots, rather than numbers in the datacards. So Olivier decided to make the input for the entire T2tt-4bd mass plane. He got batch submission sorted at Bristol, meaning I had to copy the files over to Imperial with

\begin{lstlisting}[belowskip=-0.7cm, language=sh, numbers=none]
rsync --exclude=\*~ -vuz -rltoD  --compress-level=9 /hdfs/user/od17981/FAST-RA1/<dir> ebhal@lx01.hep.ph.ic.ac.uk:"/vols/cms/RA1/80X/MC/$(date '+%Y%m%d')_FAST-RA1_datacards_T2tt-4bd/"
\end{lstlisting}

And then I had to copy the pickle files from \emph{all} my T2tt-4bd datacard directories to each of the directories in these new FAST datacards. I could do this with

\begin{lstlisting}[belowskip=-0.7cm, language=sh, numbers=none]
cd /vols/cms/RA1/80X/MC/20171026_T2tt_4bd/AlphaStats_Nominal/
for model in SMS-T2tt_mStop-*; do cp $model/mht*.pkl /vols/cms/RA1/80X/MC/<new_dir>/${model}/; done
\end{lstlisting}

In the main directory, I also needed the files \textbf{signalModels.txt} and \textbf{configuration.txt}. I made a GitHub gist for the complete instructions in case someone like Olivier wanted to reproduce the results: \url{https://gist.github.com/eshwen/d6955bf1febfe28007376e836eb20668}.
